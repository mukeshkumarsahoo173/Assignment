{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1326547d-2b7d-4cdc-b4f5-093c998dca97",
   "metadata": {},
   "source": [
    "# Q1.What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603f7671-5b99-4ee9-969b-8b7bc523f5f9",
   "metadata": {},
   "source": [
    "* Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical data features within a specific range. It rescales the data by mapping the original values to a new range, typically between 0 and 1.\n",
    "\n",
    "* The formula to perform Min-Max scaling on a feature is:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "* Here, \"value\" represents an individual data point, \"min_value\" is the minimum value of the feature in the dataset, and \"max_value\" is the maximum value of the feature in the dataset.\n",
    "\n",
    "* Min-Max scaling is useful when the original range of the data is large and you want to bring all the features to a common scale. It can help to mitigate the dominance of features with larger values, prevent bias towards certain features, and make the data more suitable for certain machine learning algorithms that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99504347-e711-4e43-8f73-f3508bf3b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled values for data point 1: [0. 0.]\n",
      "Scaled values for data point 2: [0.11111111 0.11111111]\n",
      "Scaled values for data point 3: [0.22222222 0.22222222]\n",
      "Scaled values for data point 4: [0.33333333 0.33333333]\n",
      "Scaled values for data point 5: [0.44444444 0.44444444]\n",
      "Scaled values for data point 6: [0.55555556 0.55555556]\n",
      "Scaled values for data point 7: [0.66666667 0.66666667]\n",
      "Scaled values for data point 8: [0.77777778 0.77777778]\n",
      "Scaled values for data point 9: [0.88888889 0.88888889]\n",
      "Scaled values for data point 10: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[500, 100000],\n",
    "        [1000, 200000],\n",
    "        [1500, 300000],\n",
    "        [2000, 400000],\n",
    "        [2500, 500000],\n",
    "        [3000, 600000],\n",
    "        [3500, 700000],\n",
    "        [4000, 800000],\n",
    "        [4500, 900000],\n",
    "        [5000, 1000000]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "for i in range(len(scaled_data)):\n",
    "    print(f\"Scaled values for data point {i+1}: {scaled_data[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bb30c-ec3f-4443-a3e6-538c2fa4ce22",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de73d9d-7ee2-4935-a0a9-335f54a1823d",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, is a feature scaling method that rescales the values of a feature to have a magnitude of 1. It aims to normalize the feature vectors to the unit length by dividing each data point by its Euclidean norm.\n",
    "\n",
    "The formula to perform Unit Vector scaling on a feature is:\n",
    "\n",
    "unit_vector = value / norm\n",
    "\n",
    "Here, \"value\" represents an individual data point, and \"norm\" is the Euclidean norm of the data point, calculated as the square root of the sum of the squared values.\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling focuses on the direction of the data points, whereas Min-Max scaling focuses on the relative positions and range of the values.\n",
    "\n",
    "An example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose you have a dataset of customer reviews, where each review is represented by two features: \"positive_words\" and \"negative_words.\" The \"positive_words\" feature represents the count of positive words in a review, ranging from 0 to a maximum value of 100. The \"negative_words\" feature represents the count of negative words in a review, ranging from 0 to a maximum value of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a208f199-0d40-4896-adaa-da6b810ff22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled values for data point 1: [0.4472136  0.89442719]\n",
      "Scaled values for data point 2: [0.13216372 0.9912279 ]\n",
      "Scaled values for data point 3: [0.8479983  0.52999894]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 100],\n",
    "                 [20, 150],\n",
    "                 [80, 50]])\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = normalize(data)\n",
    "# Print the scaled data\n",
    "for i in range(len(scaler)):\n",
    "    print(f\"Scaled values for data point {i+1}: {scaler[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6653c-56b9-4b47-9e4f-41d8ad4b5642",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb085021-3804-479f-903e-87f12a228851",
   "metadata": {},
   "source": [
    "* PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction and data analysis. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the essential structure and variability of the original data.\n",
    "\n",
    "*  PCA, the high-dimensional data is transformed into a set of orthogonal linearly uncorrelated variables called principal components. These principal components are ordered in terms of their explained variance, with the first principal component capturing the maximum amount of variance in the data. Subsequent principal components capture decreasing amounts of variance.\n",
    "\n",
    "* PCA helps in reducing the dimensionality of the dataset by selecting a smaller number of principal components that explain a significant portion of the total variance. By discarding the less important components, PCA can simplify the data representation, remove noise, and facilitate further analysis or visualization.\n",
    "\n",
    "* Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "* Suppose you have a dataset with five numerical features: \"Feature1,\" \"Feature2,\" \"Feature3,\" \"Feature4,\" and \"Feature5.\" The dataset has 1000 data points, and you want to reduce the dimensionality to two principal components for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069961f-e051-47d1-9d6d-b92ce9aa1285",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for FeatureExtraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1995e-cdd3-44c8-88ea-6718fe280983",
   "metadata": {},
   "source": [
    "* PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to transform a high-dimensional dataset into a lower-dimensional representation, while preserving the essential information and structure of the original data.\n",
    "\n",
    "* In the context of feature extraction, PCA can help identify the most informative features or combinations of features that contribute the most to the variability in the dataset. It achieves this by transforming the original features into a set of new features, called principal components, which are linear combinations of the original features. These principal components capture the most significant patterns and variations in the data.\n",
    "\n",
    "* By using PCA for feature extraction, we can reduce the dimensionality of the dataset while retaining the most important information. This reduction in dimensionality can improve computational efficiency, reduce noise, and mitigate the curse of dimensionality. The extracted features can then be used for further analysis, modeling, or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d7581b7-ebff-4117-9926-7946599f1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for data point 1: [-2.68412563  0.31939725]\n",
      "Extracted features for data point 2: [-2.71414169 -0.17700123]\n",
      "Extracted features for data point 3: [-2.88899057 -0.14494943]\n",
      "Extracted features for data point 4: [-2.74534286 -0.31829898]\n",
      "Extracted features for data point 5: [-2.72871654  0.32675451]\n",
      "Extracted features for data point 6: [-2.28085963  0.74133045]\n",
      "Extracted features for data point 7: [-2.82053775 -0.08946138]\n",
      "Extracted features for data point 8: [-2.62614497  0.16338496]\n",
      "Extracted features for data point 9: [-2.88638273 -0.57831175]\n",
      "Extracted features for data point 10: [-2.6727558  -0.11377425]\n",
      "Extracted features for data point 11: [-2.50694709  0.6450689 ]\n",
      "Extracted features for data point 12: [-2.61275523  0.01472994]\n",
      "Extracted features for data point 13: [-2.78610927 -0.235112  ]\n",
      "Extracted features for data point 14: [-3.22380374 -0.51139459]\n",
      "Extracted features for data point 15: [-2.64475039  1.17876464]\n",
      "Extracted features for data point 16: [-2.38603903  1.33806233]\n",
      "Extracted features for data point 17: [-2.62352788  0.81067951]\n",
      "Extracted features for data point 18: [-2.64829671  0.31184914]\n",
      "Extracted features for data point 19: [-2.19982032  0.87283904]\n",
      "Extracted features for data point 20: [-2.5879864   0.51356031]\n",
      "Extracted features for data point 21: [-2.31025622  0.39134594]\n",
      "Extracted features for data point 22: [-2.54370523  0.43299606]\n",
      "Extracted features for data point 23: [-3.21593942  0.13346807]\n",
      "Extracted features for data point 24: [-2.30273318  0.09870885]\n",
      "Extracted features for data point 25: [-2.35575405 -0.03728186]\n",
      "Extracted features for data point 26: [-2.50666891 -0.14601688]\n",
      "Extracted features for data point 27: [-2.46882007  0.13095149]\n",
      "Extracted features for data point 28: [-2.56231991  0.36771886]\n",
      "Extracted features for data point 29: [-2.63953472  0.31203998]\n",
      "Extracted features for data point 30: [-2.63198939 -0.19696122]\n",
      "Extracted features for data point 31: [-2.58739848 -0.20431849]\n",
      "Extracted features for data point 32: [-2.4099325   0.41092426]\n",
      "Extracted features for data point 33: [-2.64886233  0.81336382]\n",
      "Extracted features for data point 34: [-2.59873675  1.09314576]\n",
      "Extracted features for data point 35: [-2.63692688 -0.12132235]\n",
      "Extracted features for data point 36: [-2.86624165  0.06936447]\n",
      "Extracted features for data point 37: [-2.62523805  0.59937002]\n",
      "Extracted features for data point 38: [-2.80068412  0.26864374]\n",
      "Extracted features for data point 39: [-2.98050204 -0.48795834]\n",
      "Extracted features for data point 40: [-2.59000631  0.22904384]\n",
      "Extracted features for data point 41: [-2.77010243  0.26352753]\n",
      "Extracted features for data point 42: [-2.84936871 -0.94096057]\n",
      "Extracted features for data point 43: [-2.99740655 -0.34192606]\n",
      "Extracted features for data point 44: [-2.40561449  0.18887143]\n",
      "Extracted features for data point 45: [-2.20948924  0.43666314]\n",
      "Extracted features for data point 46: [-2.71445143 -0.2502082 ]\n",
      "Extracted features for data point 47: [-2.53814826  0.50377114]\n",
      "Extracted features for data point 48: [-2.83946217 -0.22794557]\n",
      "Extracted features for data point 49: [-2.54308575  0.57941002]\n",
      "Extracted features for data point 50: [-2.70335978  0.10770608]\n",
      "Extracted features for data point 51: [1.28482569 0.68516047]\n",
      "Extracted features for data point 52: [0.93248853 0.31833364]\n",
      "Extracted features for data point 53: [1.46430232 0.50426282]\n",
      "Extracted features for data point 54: [ 0.18331772 -0.82795901]\n",
      "Extracted features for data point 55: [1.08810326 0.07459068]\n",
      "Extracted features for data point 56: [ 0.64166908 -0.41824687]\n",
      "Extracted features for data point 57: [1.09506066 0.28346827]\n",
      "Extracted features for data point 58: [-0.74912267 -1.00489096]\n",
      "Extracted features for data point 59: [1.04413183 0.2283619 ]\n",
      "Extracted features for data point 60: [-0.0087454  -0.72308191]\n",
      "Extracted features for data point 61: [-0.50784088 -1.26597119]\n",
      "Extracted features for data point 62: [ 0.51169856 -0.10398124]\n",
      "Extracted features for data point 63: [ 0.26497651 -0.55003646]\n",
      "Extracted features for data point 64: [ 0.98493451 -0.12481785]\n",
      "Extracted features for data point 65: [-0.17392537 -0.25485421]\n",
      "Extracted features for data point 66: [0.92786078 0.46717949]\n",
      "Extracted features for data point 67: [ 0.66028376 -0.35296967]\n",
      "Extracted features for data point 68: [ 0.23610499 -0.33361077]\n",
      "Extracted features for data point 69: [ 0.94473373 -0.54314555]\n",
      "Extracted features for data point 70: [ 0.04522698 -0.58383438]\n",
      "Extracted features for data point 71: [ 1.11628318 -0.08461685]\n",
      "Extracted features for data point 72: [ 0.35788842 -0.06892503]\n",
      "Extracted features for data point 73: [ 1.29818388 -0.32778731]\n",
      "Extracted features for data point 74: [ 0.92172892 -0.18273779]\n",
      "Extracted features for data point 75: [0.71485333 0.14905594]\n",
      "Extracted features for data point 76: [0.90017437 0.32850447]\n",
      "Extracted features for data point 77: [1.33202444 0.24444088]\n",
      "Extracted features for data point 78: [1.55780216 0.26749545]\n",
      "Extracted features for data point 79: [ 0.81329065 -0.1633503 ]\n",
      "Extracted features for data point 80: [-0.30558378 -0.36826219]\n",
      "Extracted features for data point 81: [-0.06812649 -0.70517213]\n",
      "Extracted features for data point 82: [-0.18962247 -0.68028676]\n",
      "Extracted features for data point 83: [ 0.13642871 -0.31403244]\n",
      "Extracted features for data point 84: [ 1.38002644 -0.42095429]\n",
      "Extracted features for data point 85: [ 0.58800644 -0.48428742]\n",
      "Extracted features for data point 86: [0.80685831 0.19418231]\n",
      "Extracted features for data point 87: [1.22069088 0.40761959]\n",
      "Extracted features for data point 88: [ 0.81509524 -0.37203706]\n",
      "Extracted features for data point 89: [ 0.24595768 -0.2685244 ]\n",
      "Extracted features for data point 90: [ 0.16641322 -0.68192672]\n",
      "Extracted features for data point 91: [ 0.46480029 -0.67071154]\n",
      "Extracted features for data point 92: [ 0.8908152  -0.03446444]\n",
      "Extracted features for data point 93: [ 0.23054802 -0.40438585]\n",
      "Extracted features for data point 94: [-0.70453176 -1.01224823]\n",
      "Extracted features for data point 95: [ 0.35698149 -0.50491009]\n",
      "Extracted features for data point 96: [ 0.33193448 -0.21265468]\n",
      "Extracted features for data point 97: [ 0.37621565 -0.29321893]\n",
      "Extracted features for data point 98: [0.64257601 0.01773819]\n",
      "Extracted features for data point 99: [-0.90646986 -0.75609337]\n",
      "Extracted features for data point 100: [ 0.29900084 -0.34889781]\n",
      "Extracted features for data point 101: [ 2.53119273 -0.00984911]\n",
      "Extracted features for data point 102: [ 1.41523588 -0.57491635]\n",
      "Extracted features for data point 103: [2.61667602 0.34390315]\n",
      "Extracted features for data point 104: [ 1.97153105 -0.1797279 ]\n",
      "Extracted features for data point 105: [ 2.35000592 -0.04026095]\n",
      "Extracted features for data point 106: [3.39703874 0.55083667]\n",
      "Extracted features for data point 107: [ 0.52123224 -1.19275873]\n",
      "Extracted features for data point 108: [2.93258707 0.3555    ]\n",
      "Extracted features for data point 109: [ 2.32122882 -0.2438315 ]\n",
      "Extracted features for data point 110: [2.91675097 0.78279195]\n",
      "Extracted features for data point 111: [1.66177415 0.24222841]\n",
      "Extracted features for data point 112: [ 1.80340195 -0.21563762]\n",
      "Extracted features for data point 113: [2.1655918  0.21627559]\n",
      "Extracted features for data point 114: [ 1.34616358 -0.77681835]\n",
      "Extracted features for data point 115: [ 1.58592822 -0.53964071]\n",
      "Extracted features for data point 116: [1.90445637 0.11925069]\n",
      "Extracted features for data point 117: [1.94968906 0.04194326]\n",
      "Extracted features for data point 118: [3.48705536 1.17573933]\n",
      "Extracted features for data point 119: [3.79564542 0.25732297]\n",
      "Extracted features for data point 120: [ 1.30079171 -0.76114964]\n",
      "Extracted features for data point 121: [2.42781791 0.37819601]\n",
      "Extracted features for data point 122: [ 1.19900111 -0.60609153]\n",
      "Extracted features for data point 123: [3.49992004 0.4606741 ]\n",
      "Extracted features for data point 124: [ 1.38876613 -0.20439933]\n",
      "Extracted features for data point 125: [2.2754305  0.33499061]\n",
      "Extracted features for data point 126: [2.61409047 0.56090136]\n",
      "Extracted features for data point 127: [ 1.25850816 -0.17970479]\n",
      "Extracted features for data point 128: [ 1.29113206 -0.11666865]\n",
      "Extracted features for data point 129: [ 2.12360872 -0.20972948]\n",
      "Extracted features for data point 130: [2.38800302 0.4646398 ]\n",
      "Extracted features for data point 131: [2.84167278 0.37526917]\n",
      "Extracted features for data point 132: [3.23067366 1.37416509]\n",
      "Extracted features for data point 133: [ 2.15943764 -0.21727758]\n",
      "Extracted features for data point 134: [ 1.44416124 -0.14341341]\n",
      "Extracted features for data point 135: [ 1.78129481 -0.49990168]\n",
      "Extracted features for data point 136: [3.07649993 0.68808568]\n",
      "Extracted features for data point 137: [2.14424331 0.1400642 ]\n",
      "Extracted features for data point 138: [1.90509815 0.04930053]\n",
      "Extracted features for data point 139: [ 1.16932634 -0.16499026]\n",
      "Extracted features for data point 140: [2.10761114 0.37228787]\n",
      "Extracted features for data point 141: [2.31415471 0.18365128]\n",
      "Extracted features for data point 142: [1.9222678  0.40920347]\n",
      "Extracted features for data point 143: [ 1.41523588 -0.57491635]\n",
      "Extracted features for data point 144: [2.56301338 0.2778626 ]\n",
      "Extracted features for data point 145: [2.41874618 0.3047982 ]\n",
      "Extracted features for data point 146: [1.94410979 0.1875323 ]\n",
      "Extracted features for data point 147: [ 1.52716661 -0.37531698]\n",
      "Extracted features for data point 148: [1.76434572 0.07885885]\n",
      "Extracted features for data point 149: [1.90094161 0.11662796]\n",
      "Extracted features for data point 150: [ 1.39018886 -0.28266094]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Perform PCA for feature extraction\n",
    "extracted_features = pca.fit_transform(X)\n",
    "\n",
    "# Print the extracted features\n",
    "for i in range(len(extracted_features)):\n",
    "    print(f\"Extracted features for data point {i+1}: {extracted_features[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f325c8-21ad-4e7e-98bd-37522f52e00b",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245992a-1080-4273-bd31-c38babd91912",
   "metadata": {},
   "source": [
    " In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data before feeding it into the recommendation algorithm. Min-Max scaling is a common technique for normalizing numerical features within a specific range, typically between 0 and 1.\n",
    "\n",
    " Here's how you could use Min-Max scaling to preprocess the data for your recommendation system:\n",
    "\n",
    "* Identify the relevant numerical features: In this case, you mentioned price, rating, and delivery time as the features in your dataset.\n",
    "\n",
    "* Split the dataset: Split the dataset into the feature matrix (X) and the target variable (if applicable).\n",
    "\n",
    "* Apply Min-Max scaling: Initialize the MinMaxScaler from a library like scikit-learn and apply it to the feature matrix (X). This will scale each feature individually to the range [0, 1] based on the minimum and maximum values of each feature.\n",
    "\n",
    "* Fit and transform the data: Fit the MinMaxScaler on the feature matrix (X) to compute the minimum and maximum values of each feature. Then, transform the feature matrix using the computed scaling parameters.\n",
    "\n",
    "* se the preprocessed data: The preprocessed data, with features scaled using Min-Max scaling, can now be used as input for your recommendation system. The scaled features will have a consistent range, allowing for fair comparison and appropriate weighting of different features during the recommendation process.\n",
    "\n",
    "*By applying Min-Max scaling, you ensure that all numerical features are normalized within a specific range, preventing any particular feature from dominating the recommendation process solely based on its scale. This normalization step allows the recommendation algorithm to give appropriate importance to each feature and make more reliable and balanced recommendations.\n",
    "\n",
    "Remember to consider the specific requirements and characteristics of your dataset and algorithm when deciding which preprocessing techniques to apply.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466ffaf-ed56-4771-88ee-03fbc578283b",
   "metadata": {},
   "source": [
    "# 6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd210618-b847-4ccf-9fcb-f876273adcf3",
   "metadata": {},
   "source": [
    "When building a model to predict stock prices with a dataset containing numerous features, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Dimensionality reduction is beneficial in such cases because it can help mitigate the curse of dimensionality, improve model performance, and simplify the analysis of the dataset.\n",
    "\n",
    "Here's how you could use PCA to reduce the dimensionality of the stock price dataset:\n",
    "\n",
    "* Identify the relevant features: Start by identifying the features in your dataset that are related to company financial data and market trends. These features could include variables such as revenue, earnings, market capitalization, interest rates, inflation rates, or other relevant financial and economic indicators.\n",
    "\n",
    "* Preprocess the data: Before applying PCA, it is generally recommended to preprocess the data by standardizing or normalizing the features. This step ensures that the features are on a similar scale and prevents certain features from dominating the PCA process solely due to their larger magnitudes.\n",
    "\n",
    "* Apply PCA: Initialize the PCA algorithm from a library like scikit-learn and specify the desired number of components or the variance ratio you want to retain. The number of components determines the dimensionality of the resulting dataset. A higher number of components retains more information but may increase computational complexity.\n",
    "\n",
    "* Fit and transform the data: Fit the PCA algorithm on the preprocessed dataset to learn the principal components. Then, transform the dataset using the learned principal components. The transformed dataset will have reduced dimensionality, with each sample represented by a lower number of principal components instead of the original features.\n",
    "\n",
    "* Evaluate the explained variance: Assess the explained variance ratio of the principal components to understand how much of the original variance in the dataset is retained by each component. This information can guide you in determining the appropriate number of components to retain based on the desired level of variance preservation.\n",
    "\n",
    "* Use the reduced dataset for modeling: The reduced dataset, obtained by applying PCA and selecting a subset of principal components, can now be used as input for your stock price prediction model. The reduced dimensionality can help mitigate the curse of dimensionality, improve model training speed, and potentially enhance the model's ability to capture meaningful patterns and relationships in the data.\n",
    "\n",
    "It's important to note that while PCA can reduce the dimensionality of the dataset, it does not guarantee improved predictive performance. The impact of dimensionality reduction on model performance may vary depending on the specific dataset and the modeling algorithm used. It's always recommended to evaluate the performance of the reduced dataset using appropriate evaluation metrics and compare it with the performance of the original dataset to assess the effectiveness of the dimensionality reduction technique.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48ff0f-4f9b-4caf-954f-0d91f3117b65",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "842983e4-519f-4953-9a15-f9a8028c0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e8e2a-282e-42da-8a2e-b3695cbfbafa",
   "metadata": {},
   "source": [
    "To determine the number of principal components to retain during feature extraction using PCA, we need to consider the explained variance ratio of the principal components. The explained variance ratio indicates the proportion of the total variance in the dataset that is explained by each principal component. By choosing a subset of principal components that captures a significant amount of variance, we can reduce the dimensionality of the dataset while retaining most of the important information.\n",
    "\n",
    "In order to determine the number of principal components to retain, we typically look at the cumulative explained variance ratio. This is the sum of the explained variance ratios of the principal components, starting from the first component. The cumulative explained variance ratio provides insights into how much of the variance is explained as we include more principal components.\n",
    "\n",
    "Here's how you can approach determining the number of principal components to retain using PCA:\n",
    "\n",
    "Preprocess the data: Before applying PCA, preprocess the dataset by standardizing or normalizing the features. This step ensures that the features are on a similar scale and prevents certain features from dominating the PCA process solely due to their larger magnitudes.\n",
    "\n",
    "Apply PCA: Initialize the PCA algorithm and fit it on the preprocessed dataset. By default, PCA retains all the principal components.\n",
    "\n",
    "Evaluate the explained variance ratio: Access the explained_variance_ratio_ attribute of the fitted PCA object. This attribute provides the explained variance ratio of each principal component.\n",
    "\n",
    "Calculate the cumulative explained variance ratio: Calculate the cumulative sum of the explained variance ratios.\n",
    "\n",
    "Determine the number of principal components to retain: Look for the point where the cumulative explained variance ratio reaches a satisfactory level. There is no fixed threshold for this decision, as it depends on the specific dataset and the desired level of information preservation. Generally, a cumulative explained variance ratio of around 80% to 95% is often considered acceptable, but this can vary based on the specific context and requirements of your analysis.\n",
    "\n",
    "By examining the explained variance ratios and the cumulative explained variance ratio, you can decide on the number of principal components to retain. It's important to strike a balance between reducing the dimensionality of the dataset and preserving an adequate amount of variance and information.\n",
    "\n",
    "Keep in mind that the number of principal components to retain is a subjective decision and may require experimentation and analysis based on the specific dataset and problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
