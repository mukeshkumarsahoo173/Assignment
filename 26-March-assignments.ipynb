{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa9f2e0-aaeb-4fd2-bf8a-6a9df387d983",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60a1c6-307d-49c2-a4c7-83aa54d5c87d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple Linear Regression:\n",
    "* Simple linear regression is a statistical technique that models the relationship between two variables: an independent variable (predictor) and a dependent variable (response). It assumes a linear relationship between the predictor and the response variable. The goal of simple linear regression is to fit a line that best represents the relationship between the variables. The line is determined by estimating the slope and intercept of the line based on the given data.\n",
    "\n",
    "### Example of Simple Linear Regression:\n",
    "* Let's consider a simple example of simple linear regression using the relationship between the number of hours studied (independent variable) and the exam score (dependent variable). We collect data from several students, recording the number of hours they studied and their corresponding exam scores. We can then use simple linear regression to model the relationship between the number of hours studied and the exam scores. The model will provide us with a line that represents the best fit for the data, allowing us to predict the exam scores based on the number of hours studied.\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "* Multiple linear regression is an extension of simple linear regression that models the relationship between multiple independent variables and a dependent variable. Instead of considering just one predictor, multiple linear regression incorporates several predictors to understand their combined influence on the response variable. The technique assumes a linear relationship between the predictors and the response, but it accounts for the influence of multiple variables simultaneously.\n",
    "\n",
    "### Example of Multiple Linear Regression:\n",
    "* Suppose we want to predict housing prices based on various factors such as the size of the house, the number of bedrooms, and the distance from the city center. In this case, multiple linear regression can be employed. We collect data on different houses, recording the size, number of bedrooms, distance from the city center, and their corresponding sale prices. By using multiple linear regression, we can build a model that takes into account all these factors to predict the sale price of a house. The model will provide coefficients for each predictor, indicating the strength and direction of their impact on the sale price.\n",
    "\n",
    "In summary, simple linear regression is used when we have one independent variable, while multiple linear regression is employed when we have two or more independent variables. Both techniques aim to model the relationship between predictors and a response variable, but multiple linear regression allows for a more comprehensive analysis by considering multiple predictors simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03ad91-3e69-4b66-b8b5-cd200067b846",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a2f63-344c-4eb1-9d00-7de16cc043e8",
   "metadata": {},
   "source": [
    "Linear regression relies on several assumptions for accurate and reliable results. Here are the key assumptions of linear regression:\n",
    "\n",
    "1. Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear. This means that the change in the dependent variable is directly proportional to the change in the independent variables. This assumption can be assessed by plotting the data and examining if the relationship appears to be linear. Additionally, a residual plot can be used to check for linearity by examining if the residuals are randomly scattered around zero.\n",
    "\n",
    "2. Independence of errors: The errors or residuals are assumed to be independent of each other. In other words, the error term for one observation should not provide any information about the error term for another observation. This assumption can be evaluated by examining the autocorrelation of residuals using techniques like the Durbin-Watson test or by plotting the residuals against the order of observation to check for any patterns.\n",
    "\n",
    "3. Homoscedasticity: The variability of the errors or residuals should be constant across all levels of the independent variables. Homoscedasticity implies that the spread of the residuals is consistent throughout the range of the predictors. A scatter plot of residuals against predicted values can help assess homoscedasticity. If the spread of the residuals appears to increase or decrease as the predicted values change, it suggests heteroscedasticity.\n",
    "\n",
    "4. Normality of residuals: The residuals are assumed to be normally distributed. This assumption is necessary for conducting hypothesis tests, constructing confidence intervals, and making predictions. Normality of residuals can be examined through a histogram or a Q-Q plot. If the residuals deviate significantly from a normal distribution, transformations or other techniques may be applied to address the issue.\n",
    "\n",
    "5. No multicollinearity: The independent variables should not be highly correlated with each other. Multicollinearity can cause issues with the estimation of regression coefficients and lead to unstable and unreliable results. Variance Inflation Factor (VIF) or correlation matrices can be used to assess multicollinearity among the predictors.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can perform the following diagnostic tests and evaluations:\n",
    "\n",
    "1. Visual inspection: Plotting the data, including scatter plots of independent variables against the dependent variable and residual plots, can provide insights into linearity, homoscedasticity, and potential outliers.\n",
    "\n",
    "2. Statistical tests: Various statistical tests can be used to assess assumptions, such as the Durbin-Watson test for autocorrelation, tests for normality (e.g., Shapiro-Wilk test, Kolmogorov-Smirnov test), and tests for multicollinearity (e.g., VIF calculation).\n",
    "\n",
    "3. Residual analysis: Examining the residuals for patterns, such as non-linearity or heteroscedasticity, through residual plots, can help identify violations of assumptions.\n",
    "\n",
    "4. Outlier detection: Identifying and analyzing potential outliers in the data can impact the assumptions and regression results. Techniques like leverage analysis, Cook's distance, or studentized residuals can aid in detecting outliers.\n",
    "\n",
    "5. Transformation: If the assumptions are violated, applying transformations (e.g., logarithmic, square root) to the variables may help meet the assumptions or using robust regression techniques that are less sensitive to assumption violations.\n",
    "\n",
    "It is important to note that linear regression assumptions should be evaluated and addressed appropriately based on the specific dataset and research context to ensure reliable and valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372340a7-7968-4fd9-ace7-0336cb52a1e3",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcd4ed-3a47-4fc7-983f-06b47af16d4a",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept represent the relationship between the independent variable(s) and the dependent variable. Here's how you can interpret them:\n",
    "\n",
    "* Intercept (y-intercept):\n",
    "The intercept (often denoted as β₀ or b₀) represents the predicted value of the dependent variable when all independent variables are zero. It indicates the starting point or the value of the dependent variable when the independent variable has no effect. In other words, it represents the baseline or the value of the dependent variable when all predictors are absent.\n",
    "Example interpretation: Let's consider a real-world scenario where we are analyzing the housing prices based on the size of the house (in square feet). The intercept term in the linear regression model represents the predicted housing price when the size of the house is zero. Since a house cannot have zero size, the interpretation of the intercept is not practically meaningful in this case.\n",
    "\n",
    "* Slope (coefficients):\n",
    "The slope (often denoted as β₁, β₂, etc., or b₁, b₂, etc.) represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable while holding other variables constant. It quantifies the rate of change in the dependent variable for each unit change in the independent variable.\n",
    "Example interpretation: Continuing with the housing price example, let's say the estimated slope coefficient for the size of the house (in square feet) is 50. This means that, on average, for every additional square foot increase in the size of the house, the predicted housing price increases by $50, assuming other factors are held constant. So, a house that is 100 square feet larger is expected to have a predicted price that is $5,000 higher.\n",
    "\n",
    "It's important to consider the context and the specific units of the independent and dependent variables when interpreting the slope. Additionally, if multiple independent variables are included in the model, each slope coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1364a8d-cf01-452e-a58d-aa833c23f38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (y-intercept): 63555.66700100305\n",
      "Slope (coefficient for house size): 121.0631895687061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "house_sizes = np.array([[1200], [1500], [1800], [1000], [1350], [1600], [2000]])\n",
    "prices = np.array([200000, 250000, 280000, 180000, 240000, 260000, 300000])\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(house_sizes, prices)\n",
    "\n",
    "# Get the intercept and slope\n",
    "intercept = model.intercept_\n",
    "slope = model.coef_[0]\n",
    "\n",
    "# Interpretation\n",
    "print(\"Intercept (y-intercept):\", intercept)\n",
    "print(\"Slope (coefficient for house size):\", slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a0125-a653-4f28-89f3-078d1c03b245",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09aec61-ef86-426c-9b1f-980ccb74e698",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative optimization algorithm used to minimize the cost function or error function of a machine learning model. It is a widely used method in training various types of models, including linear regression, logistic regression, neural networks, and more.\n",
    "\n",
    "The concept of gradient descent revolves around finding the optimal values for the parameters (weights or coefficients) of a model by iteratively updating them based on the gradient (slope) of the cost function. The goal is to reach the minimum of the cost function, which corresponds to the best-fit solution or optimal model parameters.\n",
    "\n",
    "Here's how gradient descent works in machine learning:\n",
    "\n",
    "1.  Initialization: Start by initializing the model parameters randomly or with some predefined values.\n",
    "\n",
    "2. Forward Propagation: Perform a forward pass through the model to compute the predicted output or the hypothesis function based on the current parameter values.\n",
    "\n",
    "3. Calculate the Cost: Compare the predicted output with the actual output and calculate the cost or error. The cost function quantifies how well the model is performing.\n",
    "\n",
    "4. Backward Propagation (Gradient Calculation): Compute the gradients (partial derivatives) of the cost function with respect to each parameter. The gradients indicate the direction and magnitude of the steepest increase or decrease in the cost function.\n",
    "\n",
    "5. Parameter Update: Update the parameters by subtracting a fraction of the gradients from the current parameter values. This fraction is determined by the learning rate, which controls the step size of the parameter update. The learning rate should be carefully chosen to ensure convergence and avoid overshooting or getting stuck in local minima.\n",
    "\n",
    "6. Repeat Steps 2-5: Iterate the process by performing forward propagation, calculating the cost, computing the gradients, and updating the parameters until a stopping criterion is met. The stopping criterion can be a maximum number of iterations, achieving a desired level of accuracy, or reaching a predefined threshold for the cost function.\n",
    "\n",
    "7. Model Evaluation: After convergence or reaching the stopping criterion, evaluate the trained model's performance on a separate test dataset or through other evaluation metrics.\n",
    "\n",
    "Gradient descent helps in finding the optimal parameters of a model by iteratively adjusting them in the direction of steepest descent in the cost function's landscape. By continuously updating the parameters based on the gradients, the algorithm gradually converges towards the minimum of the cost function, leading to an optimized model.\n",
    "\n",
    "It's worth noting that there are variations of gradient descent, such as batch gradient descent, stochastic gradient descent (SGD), and mini-batch gradient descent. These variations differ in how they update the parameters and utilize the training data, making them suitable for different scenarios and trade-offs between computation efficiency and convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056d7c1-ad9f-47fc-a463-a745580c3409",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40569c07-75f5-435e-96ee-7e3acbebfcd8",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and multiple independent variables. In multiple linear regression, the dependent variable is predicted based on the linear combination of two or more independent variables, taking into account their individual effects on the dependent variable while holding other variables constant.\n",
    "\n",
    "Here are the key differences between multiple linear regression and simple linear regression:\n",
    "\n",
    "1. Number of Independent Variables:\n",
    "In simple linear regression, there is only one independent variable (predictor variable) used to predict the dependent variable. Multiple linear regression, on the other hand, involves two or more independent variables to predict the dependent variable.\n",
    "\n",
    "2. Model Equation:\n",
    "In simple linear regression, the model equation is of the form:\n",
    "y = β₀ + β₁x\n",
    "where y is the dependent variable, x is the independent variable, β₀ is the intercept, and β₁ is the slope coefficient.\n",
    "\n",
    "In multiple linear regression, the model equation is expanded to include multiple independent variables:\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ\n",
    "where x₁, x₂, ..., xₚ are the independent variables, β₀ is the intercept, and β₁, β₂, ..., βₚ are the respective slope coefficients for each independent variable.\n",
    "\n",
    "1. Interpretation of Coefficients:\n",
    "In simple linear regression, the slope coefficient (β₁) represents the change in the dependent variable for a one-unit change in the independent variable, while holding other variables constant.\n",
    "In multiple linear regression, each slope coefficient (β₁, β₂, ..., βₚ) represents the change in the dependent variable associated with a one-unit change in the respective independent variable, while holding all other variables constant. This allows for analyzing the individual effects of each independent variable on the dependent variable while accounting for the presence of other variables.\n",
    "\n",
    "2. Model Complexity:\n",
    "Simple linear regression is a simpler model as it involves only one independent variable. It can be useful when examining the relationship between two variables in isolation. However, it may not capture the full complexity of real-world scenarios where multiple factors influence the dependent variable.\n",
    "Multiple linear regression provides a more comprehensive model by incorporating multiple independent variables. It allows for considering the joint effects of several variables on the dependent variable and provides a more realistic representation of complex relationships.\n",
    "\n",
    "1. Model Assumptions:\n",
    "Both simple and multiple linear regression rely on similar assumptions, such as linearity, independence of errors, homoscedasticity, normality of residuals, and no multicollinearity. However, multiple linear regression involves additional considerations due to the presence of multiple independent variables, such as multicollinearity between predictors.\n",
    "Overall, multiple linear regression expands the capabilities of simple linear regression by allowing for the analysis of the relationship between a dependent variable and multiple independent variables simultaneously. It provides a more flexible and realistic approach to modeling complex relationships in various fields, including economics, social sciences, and data analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e2c24-d43a-4543-80b0-8790cab8837e",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9cd46-4470-40ac-a361-033bd0ec213a",
   "metadata": {},
   "source": [
    "Multicollinearity refers to the presence of high correlation or linear dependence among the independent variables in a multiple linear regression model. It can cause several issues, including unstable and unreliable coefficient estimates, difficulty in interpreting the individual effects of variables, and challenges in identifying the true relationship between the independent variables and the dependent variable.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "There are several ways to detect multicollinearity in a multiple linear regression model:\n",
    "\n",
    "1. Correlation Matrix: Calculate the correlation matrix among the independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "2. Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficient is inflated due to multicollinearity. VIF values greater than 1 indicate the presence of multicollinearity, with higher values indicating a stronger effect.\n",
    "\n",
    "3. Tolerance: Calculate the tolerance for each independent variable, which is the reciprocal of the VIF. Tolerance values close to 0 indicate high multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "If multicollinearity is detected in a multiple linear regression model, there are several strategies to address this issue:\n",
    "\n",
    "1. Feature Selection: Remove one or more independent variables that are highly correlated with each other or have high VIF values. Prioritize keeping variables that have a stronger theoretical or practical justification for inclusion in the model.\n",
    "\n",
    "2. Data Collection: Collect more data to reduce the correlation among the independent variables. Increasing the sample size can help alleviate multicollinearity issues.\n",
    "\n",
    "3. Variable Transformation: Transform the variables to reduce the multicollinearity. For example, you can use dimensionality reduction techniques like principal component analysis (PCA) or factor analysis to create new uncorrelated variables from the original ones.\n",
    "\n",
    "4. Ridge Regression: Use regularization techniques like ridge regression that penalize the regression coefficients. Ridge regression introduces a bias in the estimation of coefficients, which can help mitigate the effects of multicollinearity.\n",
    "\n",
    "5. Domain Knowledge: Leverage your domain knowledge to understand the variables and their relationships better. Sometimes, high correlations between variables may be expected or explainable based on the underlying context. In such cases, it may be reasonable to retain the correlated variables in the model.\n",
    "\n",
    "It's important to note that the choice of addressing multicollinearity depends on the specific context, goals of the analysis, and the impact on the interpretation of the model. Before applying any remedial measures, it's crucial to thoroughly understand the data and consider the potential consequences of removing or transforming variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e4b14-25d3-456b-81e4-63f035cd4abc",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29afe2-c2a7-4f9c-a498-f44d8e69170b",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between the independent variable(s) and the dependent variable as an nth-degree polynomial function. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression allows for nonlinear relationships to be captured.\n",
    "\n",
    "Here are the key aspects that differentiate polynomial regression from linear regression:\n",
    "\n",
    "1. Model Equation:\n",
    "In linear regression, the model equation is a linear combination of the independent variables:\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ\n",
    "where x₁, x₂, ..., xₚ are the independent variables, β₀ is the intercept, and β₁, β₂, ..., βₚ are the respective slope coefficients for each independent variable.\n",
    "In polynomial regression, the model equation involves powers and products of the independent variable(s) up to a specified degree (n):\n",
    "y = β₀ + β₁x + β₂x² + ... + βₙxⁿ\n",
    "where x is the independent variable, β₀ is the intercept, and β₁, β₂, ..., βₙ are the coefficients for the polynomial terms.\n",
    "\n",
    "2. Nonlinear Relationship:\n",
    "While linear regression assumes a linear relationship between the variables, polynomial regression can capture nonlinear relationships. By introducing polynomial terms (such as squared terms, cubic terms, etc.) into the model equation, polynomial regression allows for curvilinear patterns and nonlinear trends to be represented.\n",
    "\n",
    "3. Flexibility:\n",
    "Polynomial regression offers greater flexibility in fitting the data compared to linear regression. By increasing the degree of the polynomial (n), the model can fit complex and intricate relationships between the variables. However, higher degrees of polynomials can also lead to overfitting if not carefully controlled.\n",
    "\n",
    "4. Interpretation:\n",
    "In linear regression, the interpretation of coefficients is straightforward. Each coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable, holding other variables constant.\n",
    "\n",
    "In polynomial regression, the interpretation becomes more nuanced. The coefficients represent the change in the dependent variable associated with a one-unit change in the independent variable(s) for a specific degree of the polynomial term. The interpretation can vary depending on the degree of the polynomial and the interaction between the polynomial terms.\n",
    "\n",
    "5. Overfitting:\n",
    "Polynomial regression is more prone to overfitting than linear regression. As the degree of the polynomial increases, the model becomes increasingly complex and can fit the noise in the data, leading to poor generalization to unseen data. Proper model evaluation and regularization techniques are important to mitigate overfitting in polynomial regression.\n",
    "Polynomial regression provides a flexible framework to capture nonlinear relationships between variables. It can be useful when the relationship between the independent and dependent variables is not well described by a straight line. However, it requires careful consideration of the appropriate degree of the polynomial, regularization techniques, and model evaluation to ensure the model's reliability and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1086ec9b-ae1f-4fbe-bd58-9e000fe04b1e",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a313d-15ab-474a-9072-ebf459a3a3ec",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "* Capturing Nonlinear Relationships: Polynomial regression can capture complex nonlinear relationships between the independent and dependent variables. It allows for modeling curved or non-linear patterns in the data that cannot be represented by a straight line.\n",
    "\n",
    "* Flexibility: Polynomial regression offers more flexibility in fitting the data compared to linear regression. By increasing the degree of the polynomial, the model can capture more intricate relationships and adjust to the data's shape.\n",
    "\n",
    "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "* Overfitting: Polynomial regression is more prone to overfitting, especially when using high-degree polynomials. If the degree of the polynomial is too high relative to the available data, the model can fit the noise in the data, resulting in poor generalization to unseen data.\n",
    "\n",
    "* Increased Complexity: With higher degrees of polynomials, the model becomes more complex, leading to increased computational requirements and decreased interpretability of the model. It may be challenging to interpret the coefficients and extract meaningful insights from the model.\n",
    "\n",
    "Situations where Polynomial Regression is Preferred:\n",
    "\n",
    "* Nonlinear Relationships: Polynomial regression is suitable when there is a prior expectation or evidence of a nonlinear relationship between the independent and dependent variables. It allows for capturing curved or non-linear patterns in the data.\n",
    "\n",
    "* Higher Degree of Flexibility: Polynomial regression is preferred when linear regression fails to adequately fit the data or when the relationship between the variables is known or suspected to be nonlinear. It offers the flexibility to adjust to different shapes and trends in the data.\n",
    "\n",
    "* Limited Sample Size: When the sample size is small, polynomial regression can be useful. It can better capture the available data points and provide a more accurate representation of the underlying relationship.\n",
    "\n",
    "Exploratory Data Analysis: Polynomial regression can be employed during exploratory data analysis to investigate the nature of the relationship between variables. By fitting polynomials of different degrees, insights about the underlying relationship can be gained.\n",
    "\n",
    "It is important to note that the selection between linear regression and polynomial regression depends on the specific context, the nature of the data, and the relationship between variables. The choice should be based on careful consideration of the data, model evaluation techniques, and the balance between model complexity and interpretability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c12d64-6f82-4858-b08a-18f64dff2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
